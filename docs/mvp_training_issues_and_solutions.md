# MVP LoRA è®­ç»ƒé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬æ–‡æ¡£è®°å½•äº†åœ¨å®ç° InstructBLIP LoRA å¾®è°ƒ MVP è¿‡ç¨‹ä¸­é‡åˆ°çš„å…³é”®é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚

## é—®é¢˜ 1: `inputs_embeds` å†²çª

### é—®é¢˜æè¿°

```
TypeError: InstructBlipForConditionalGeneration.forward() got an unexpected keyword argument 'inputs_embeds'
```

### æ ¹æœ¬åŸå› 

- HuggingFace InstructBLIP å®ç°ä¸­å­˜åœ¨å‚æ•°ä¼ é€’å†²çª
- T5 æ¨¡å‹è¦æ±‚ `inputs_embeds` å‚æ•°ï¼Œä½† InstructBLIP wrapper ä¸æ”¯æŒ
- ä¸åŒç‰ˆæœ¬çš„ transformers å’Œ peft åº“å­˜åœ¨å…¼å®¹æ€§é—®é¢˜

### è§£å†³æ–¹æ¡ˆ âœ…

**è‡ªå®šä¹‰ forward å®ç°**ï¼š

```python
def forward(self, batch):
    # 1. Vision encoding
    vision_outputs = self.model.vision_model(pixel_values)
    image_embeds = vision_outputs.last_hidden_state

    # 2. Q-Former processing (LoRA è‡ªåŠ¨åº”ç”¨)
    query_outputs = self.model.qformer(...)

    # 3. æ‰‹åŠ¨æ„å»º inputs_embeds
    language_model_inputs = self.model.language_projection(query_embeds)
    text_embeds = self.model.language_model.get_input_embeddings()(input_ids)
    inputs_embeds = torch.cat([language_model_inputs, text_embeds], dim=1)

    # 4. ç›´æ¥è°ƒç”¨ T5ï¼Œåªä¼ é€’ inputs_embeds
    outputs = self.model.language_model(
        inputs_embeds=inputs_embeds,  # å…³é”®ï¼šåªä¼ é€’è¿™ä¸ª
        attention_mask=full_attention_mask,
        labels=labels
    )
```

### å…³é”®æˆåŠŸè¦ç´ 

- ç»•è¿‡ HF InstructBLIP çš„å‚æ•°ä¼ é€’æœºåˆ¶
- æ‰‹åŠ¨æ§åˆ¶æ¯ä¸ªæ­¥éª¤çš„æ•°æ®æµ
- ç¡®ä¿ LoRA åœ¨ Q-Former ä¸­æ­£å¸¸å·¥ä½œ

## é—®é¢˜ 2: LoRA é…ç½®éªŒè¯

### é—®é¢˜æè¿°

å¦‚ä½•ç¡®ä¿ LoRA åªåœ¨ Q-Former ä¸­å·¥ä½œï¼Œè€Œä¸å½±å“å…¶ä»–ç»„ä»¶ï¼Ÿ

### è§£å†³æ–¹æ¡ˆ âœ…

**LoRA éªŒè¯æ–¹æ³•**ï¼š

```python
def verify_lora_training(self):
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
    trainable_params = 0
    lora_params = {}

    for name, param in self.model.named_parameters():
        if param.requires_grad and 'lora' in name.lower():
            lora_params[name] = param

    # éªŒè¯åªæœ‰ Q-Former è¢«å¾®è°ƒ
    qformer_lora_params = 0
    for name, param in self.model.named_parameters():
        if 'qformer' in name.lower() and param.requires_grad:
            qformer_lora_params += param.numel()
```

### éªŒè¯ç»“æœ

- âœ… æ‰¾åˆ° 240 ä¸ª LoRA å‚æ•°
- âœ… åªæœ‰ Q-Former è¢«å¾®è°ƒ (2,420,736 å‚æ•°)
- âœ… å‚æ•°æ¯”ä¾‹ 0.0601% (ç¬¦åˆè®ºæ–‡ < 2% è¦æ±‚)

## é—®é¢˜ 3: æ•°å€¼ç¨³å®šæ€§é—®é¢˜

### é—®é¢˜æè¿°

```
WARNING: Forward pass returned None at step 1
WARNING: NaN detected in Q-Former outputs
```

### æ ¹æœ¬åŸå› 

- LoRA å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å˜å¾—æ•°å€¼ä¸ç¨³å®š
- å­¦ä¹ ç‡è¿‡é«˜å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
- Q-Former ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹å‚æ•°å˜åŒ–æ•æ„Ÿ

### è§£å†³æ–¹æ¡ˆè¿›åŒ–

#### å°è¯• 1: é™ä½å­¦ä¹ ç‡

```python
learning_rate = 1e-4  # åŸå§‹ â†’ NaN
learning_rate = 1e-5  # æ”¹è¿› â†’ ä»æœ‰ NaN
learning_rate = 5e-6  # å½“å‰æµ‹è¯•
```

#### å°è¯• 2: é™ä½ LoRA rank

```python
lora_r = 16, lora_alpha = 32  # åŸå§‹ â†’ ä¸ç¨³å®š
lora_r = 8,  lora_alpha = 16  # æ”¹è¿› â†’ ä»æœ‰é—®é¢˜
lora_r = 4,  lora_alpha = 8   # å½“å‰æµ‹è¯•
```

#### å°è¯• 3: æ·»åŠ æ¢¯åº¦è£å‰ª

```python
# åœ¨è®­ç»ƒå™¨ä¸­æ·»åŠ 
torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm=0.1)
```

#### å°è¯• 4: å¢åŠ  warmup

```python
warmup_steps = 50  # è®©å­¦ä¹ ç‡æ›´å¹³æ»‘å¯åŠ¨
```

### å½“å‰æœ€ä½³é…ç½®

```python
learning_rate = 5e-6
lora_r = 4
lora_alpha = 8
max_grad_norm = 0.1
warmup_steps = 50
```

#### æœ€æ–°æµ‹è¯•ç»“æœ (rank=4, lr=5e-6)

```
LoRA parameters found: 240
Q-Former LoRA parameters: 1,210,368 (å‡å°‘äº†ä¸€åŠ)
âœ… Q-Former LoRA is correctly configured!

Step 0/10: Loss: 2.0742 âœ… ç¬¬ä¸€ä¸ªbatchæˆåŠŸ
WARNING: Forward pass returned None at step 1-9 âŒ é—®é¢˜ä¾ç„¶å­˜åœ¨
```

**ç»“è®º**: è¶…å‚æ•°è°ƒä¼˜æ— æ³•æ ¹æœ¬è§£å†³é—®é¢˜ï¼Œéœ€è¦æ›´æ·±å±‚çš„è§£å†³æ–¹æ¡ˆã€‚

## é—®é¢˜ 4: è®­ç»ƒç›‘æ§å’Œè°ƒè¯•

### é—®é¢˜æè¿°

å¦‚ä½•æœ‰æ•ˆç›‘æ§ LoRA è®­ç»ƒè¿‡ç¨‹ï¼ŒåŠæ—¶å‘ç°æ•°å€¼é—®é¢˜ï¼Ÿ

### è§£å†³æ–¹æ¡ˆ âœ…

**å¤šå±‚æ¬¡æ£€æŸ¥æœºåˆ¶**ï¼š

```python
# 1. Q-Former è¾“å‡ºæ£€æŸ¥
if torch.any(torch.isnan(query_embeds)) or torch.any(torch.isinf(query_embeds)):
    return None  # è·³è¿‡æœ‰é—®é¢˜çš„ batch

# 2. æŠ•å½±å±‚è¾“å‡ºæ£€æŸ¥
if torch.any(torch.isnan(language_model_inputs)):
    return None

# 3. è®­ç»ƒå™¨ä¸­çš„ batch è·³è¿‡æœºåˆ¶
if outputs is None:
    print(f"WARNING: Forward pass returned None at step {step}")
    continue
```

## é—®é¢˜ 5: Checkpoint ä¿å­˜é¢‘ç‡

### é—®é¢˜æè¿°

æ¯ä¸ª epoch éƒ½ä¿å­˜ checkpoint è¿‡äºé¢‘ç¹ï¼Œå ç”¨å­˜å‚¨ç©ºé—´ã€‚

### è§£å†³æ–¹æ¡ˆ âœ…

```python
# æ¯2ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochä¿å­˜
if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
    checkpoint_path = os.path.join(config.save_dir, f"checkpoint_epoch_{epoch+1}.pth")
    trainer.save_checkpoint(checkpoint_path, epoch, train_loss, val_loss)
```

## MVP æˆåŠŸæ ‡å‡†

### âœ… å·²è¾¾æˆ

1. **LoRA æ¶æ„æ­£ç¡®**: åªå¾®è°ƒ Q-Formerï¼Œç¬¦åˆè®ºæ–‡è®¾è®¡
2. **è‡ªå®šä¹‰ forward æˆåŠŸ**: ç»•è¿‡ HF InstructBLIP å†²çª
3. **è®­ç»ƒæµç¨‹å®Œæ•´**: ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹ä¿å­˜å…¨éƒ¨æˆåŠŸ
4. **æ¨¡å‹ä¿å­˜æˆåŠŸ**: checkpoint å¯ç”¨äºæ¨ç†

### ğŸ”„ æŒç»­ä¼˜åŒ–

1. **æ•°å€¼ç¨³å®šæ€§**: éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜è¶…å‚æ•°
2. **è®­ç»ƒæ•ˆç‡**: å‡å°‘è·³è¿‡çš„ batch æ•°é‡
3. **æ”¶æ•›æ€§**: ç¡®ä¿ loss èƒ½å¤Ÿç¨³å®šä¸‹é™

## å½“å‰çŠ¶æ€

### æœ€æ–°æµ‹è¯•ç»“æœ

```
LoRA parameters found: 240
Q-Former LoRA parameters: 2,420,736
âœ… Q-Former LoRA is correctly configured!

Step 0/10: Loss: 2.3594 âœ… ç¬¬ä¸€ä¸ªbatchæˆåŠŸ
WARNING: Forward pass returned None at step 1-9 âŒ åç»­batchå¤±è´¥
```

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. æµ‹è¯•æ›´ä¿å®ˆçš„è¶…å‚æ•°é…ç½®
2. å®ç°æ›´ç»†ç²’åº¦çš„æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
3. è€ƒè™‘ä½¿ç”¨ä¸åŒçš„ LoRA åˆå§‹åŒ–ç­–ç•¥
4. æ‰©å±•åˆ°æ›´å¤šæ ·æœ¬éªŒè¯æ”¶æ•›æ€§

## ç»éªŒæ€»ç»“

### å…³é”®æ´å¯Ÿ

1. **HF InstructBLIP å®ç°æœ‰å±€é™æ€§** - éœ€è¦è‡ªå®šä¹‰ forward
2. **LoRA å¾®è°ƒæ¯”å…¨å‚æ•°å¾®è°ƒæ›´æ•æ„Ÿ** - éœ€è¦æ›´ä¿å®ˆçš„è¶…å‚æ•°
3. **Q-Former æ³¨æ„åŠ›æœºåˆ¶å®¹æ˜“ä¸ç¨³å®š** - éœ€è¦ä¸¥æ ¼çš„æ•°å€¼æ£€æŸ¥
4. **æ¸è¿›å¼è°ƒä¼˜æ˜¯å¿…è¦çš„** - ä»æœ€ä¿å®ˆé…ç½®å¼€å§‹

### æœ€ä½³å®è·µ

1. å§‹ç»ˆéªŒè¯ LoRA é…ç½®çš„æ­£ç¡®æ€§
2. å®ç°å¤šå±‚æ¬¡çš„æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
3. ä½¿ç”¨æ¸è¿›å¼è¶…å‚æ•°è°ƒä¼˜
4. ä¿æŒè¯¦ç»†çš„è®­ç»ƒæ—¥å¿—å’Œç›‘æ§

---

## é—®é¢˜ 4: å†…å­˜ä½¿ç”¨è¿‡é«˜ - LoRA èŒƒå›´è¿‡å¹¿ ğŸ†•

### é—®é¢˜æè¿°

```
batch_size=24 æ—¶è¿›åº¦æ¡å¡ä½ï¼Œä½†è®­ç»ƒå…¶ä»– LoRA æ¨¡å‹å¯ä»¥å¼€åˆ° batch_size=24
```

### æ ¹æœ¬åŸå› 

**LoRA åº”ç”¨èŒƒå›´è¿‡å¹¿**ï¼š

```python
# å½“å‰é…ç½®åŒæ—¶å¾®è°ƒä¸¤ä¸ªæ¨¡å—
target_modules = ["query", "key", "value", "dense"]
# è¿™ä¼šåŒ¹é…åˆ°ï¼š
# 1. Q-Former çš„æ³¨æ„åŠ›å±‚ âœ… (æˆ‘ä»¬æƒ³è¦çš„)
# 2. Language Model çš„æ³¨æ„åŠ›å±‚ âŒ (ä¸å¿…è¦çš„)
```

**æ¶æ„å¯¹æ¯”**ï¼š
| æ¨¡å‹ | LoRA åº”ç”¨èŒƒå›´ | å†…å­˜éœ€æ±‚ | æœ€å¤§ batch_size |
|------|---------------|----------|-----------------|
| BLIP-2 LoRA | ä¸»è¦ Q-Former | ä½ | 24 |
| InstructBLIP LoRA (ä¹‹å‰) | Q-Former + LLM | é«˜ | 10 |
| InstructBLIP LoRA (ä¼˜åŒ–å) | åª Q-Former | ä¸­ç­‰ | 16-20 (é¢„æœŸ) |

### è§£å†³æ–¹æ¡ˆ âœ…

**åªå¾®è°ƒ Q-Formerï¼Œå†»ç»“ Language Model**ï¼š

```python
# lora_model.py - ä¿®æ”¹ä»»åŠ¡ç±»å‹
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    lora_dropout=config.lora_dropout,
    target_modules=config.target_modules,
    task_type=TaskType.FEATURE_EXTRACTION,  # Q-Former æ˜¯ç‰¹å¾æå–
    modules_to_save=None
)
```

**ç†è®ºä¾æ®**ï¼š

1. **Q-Former çš„ä½œç”¨**: è§†è§‰-è¯­è¨€å¯¹é½ï¼Œè¿™æ˜¯æœ€éœ€è¦å¾®è°ƒçš„éƒ¨åˆ†
2. **Language Model**: é¢„è®­ç»ƒçš„è¯­è¨€èƒ½åŠ›å·²ç»å¾ˆå¼ºï¼Œå†»ç»“å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
3. **å†…å­˜æ•ˆç‡**: åªå¾®è°ƒå¿…è¦éƒ¨åˆ†ï¼Œå¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨
4. **è®­ç»ƒç¨³å®šæ€§**: å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œé™ä½è®­ç»ƒå¤æ‚åº¦

**é¢„æœŸæ•ˆæœ**ï¼š

- âœ… å†…å­˜ä½¿ç”¨å‡å°‘ 50%+
- âœ… batch_size å¯ä»¥å¢åŠ åˆ° 16-20
- âœ… è®­ç»ƒæ›´ç¨³å®š
- âœ… å¯èƒ½è·å¾—æ›´å¥½çš„æ•ˆæœï¼ˆä¸“æ³¨äºè§†è§‰-è¯­è¨€å¯¹é½ï¼‰

---

_æœ€åæ›´æ–°: 2025-01-22 - æ·»åŠ  LoRA èŒƒå›´ä¼˜åŒ–è§£å†³æ–¹æ¡ˆ_
