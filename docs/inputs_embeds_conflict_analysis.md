# InstructBLIP `inputs_embeds` å†²çªé—®é¢˜åˆ†æ

## é—®é¢˜æ¦‚è¿°

åœ¨ä½¿ç”¨ Hugging Face çš„ InstructBLIP æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒæ—¶ï¼Œé‡åˆ°äº† `inputs_embeds` å‚æ•°å†²çªé—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜åœ¨å¤šä¸ª transformers ç‰ˆæœ¬ä¸­éƒ½å­˜åœ¨ï¼Œä½†è¡¨ç°å½¢å¼ä¸åŒã€‚

## é—®é¢˜è¡¨ç°

### transformers 4.53.0.dev0 (å¼€å‘ç‰ˆ)

```
TypeError: T5ForConditionalGeneration got multiple values for keyword argument 'inputs_embeds'
```

- **ä½ç½®**: `modeling_instructblip.py:1645`
- **åŸå› **: T5 åŒæ—¶æ”¶åˆ°äº† `input_ids` å’Œ `inputs_embeds` å‚æ•°

### transformers 4.44.0 (ç¨³å®šç‰ˆ)

```
TypeError: InstructBlipForConditionalGeneration.forward() got an unexpected keyword argument 'inputs_embeds'
```

- **ä½ç½®**: InstructBLIP forward æ–¹æ³•è°ƒç”¨
- **åŸå› **: InstructBLIP çš„ forward æ–¹æ³•ç­¾åä¸æ¥å— `inputs_embeds` å‚æ•°

### transformers 4.37.0

```
Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper
```

- **åŸå› **: tokenizer ç‰ˆæœ¬ä¸å…¼å®¹

## å†²çªæœºåˆ¶åˆ†æ

### InstructBLIP å†…éƒ¨å·¥ä½œæµç¨‹

```python
def forward(self, pixel_values, qformer_input_ids, input_ids, labels, ...):
    # 1. å¤„ç†å›¾åƒç‰¹å¾
    image_features = self.vision_model(pixel_values)

    # 2. Q-Former å¤„ç†æŒ‡ä»¤å’Œå›¾åƒ
    query_outputs = self.qformer(
        qformer_input_ids,
        encoder_hidden_states=image_features
    )

    # 3. å…³é”®é—®é¢˜ï¼šä» input_ids ç”Ÿæˆ inputs_embeds
    inputs_embeds = self.language_model.get_input_embeddings()(input_ids)  # ç¬¬1607è¡Œ

    # 4. å°† Q-Former è¾“å‡ºä¸æ–‡æœ¬åµŒå…¥è¿æ¥
    inputs_embeds = torch.cat([
        query_outputs.last_hidden_state,
        inputs_embeds
    ], dim=1)

    # 5. è°ƒç”¨ T5 - è¿™é‡Œå‘ç”Ÿå†²çªï¼
    outputs = self.language_model(
        inputs_embeds=inputs_embeds,  # InstructBLIP ä¼ é€’çš„åµŒå…¥
        # é—®é¢˜ï¼šæŸäº›æƒ…å†µä¸‹ï¼ŒT5 ä¹Ÿæ”¶åˆ°äº† input_ids å‚æ•°
        # å¯¼è‡´ T5 å†…éƒ¨åŒæ—¶æ”¶åˆ° input_ids å’Œ inputs_embeds
        labels=labels
    )
```

### å†²çªçš„æ ¹æœ¬åŸå› 

1. **å‚æ•°ä¼ é€’å¤æ‚æ€§**: InstructBLIP éœ€è¦èåˆå¤šä¸ªè¾“å…¥æºï¼š

   - å›¾åƒç‰¹å¾ (é€šè¿‡ Q-Former)
   - æŒ‡ä»¤æ–‡æœ¬ (qformer_input_ids)
   - æç¤ºæ–‡æœ¬ (input_ids)

2. **T5 çš„å‚æ•°çº¦æŸ**: T5ForConditionalGeneration åªèƒ½æ¥å—ä»¥ä¸‹ä¹‹ä¸€ï¼š

   - `input_ids` (token IDs)
   - `inputs_embeds` (é¢„è®¡ç®—çš„åµŒå…¥)
   - ä½†ä¸èƒ½åŒæ—¶æ¥å—ä¸¤è€…

3. **ç‰ˆæœ¬é—´ API å˜åŒ–**: ä¸åŒç‰ˆæœ¬çš„ transformers å¯¹å‚æ•°å¤„ç†æ–¹å¼ä¸åŒ

## ä¸ºä»€ä¹ˆ RSGPT æ²¡æœ‰è¿™ä¸ªé—®é¢˜

RSGPT é‡‡ç”¨äº†æ‰‹åŠ¨å®ç° InstructBLIP forward æµç¨‹çš„æ–¹å¼ï¼š

```python
# RSGPT çš„æ–¹æ³•
def forward(self, samples):
    # æ‰‹åŠ¨æ§åˆ¶æ¯ä¸ªæ­¥éª¤
    image = samples["image"]
    text_input = samples["text_input"]
    text_output = samples["text_output"]

    # æ‰‹åŠ¨å¤„ç†å›¾åƒç¼–ç 
    # æ‰‹åŠ¨å¤„ç† Q-Former
    # æ‰‹åŠ¨å¤„ç†æ–‡æœ¬åµŒå…¥å’Œè¿æ¥
    # ç›´æ¥æ§åˆ¶ T5 çš„è°ƒç”¨å‚æ•°
```

è¿™ç§æ–¹å¼é¿å…äº† Hugging Face å®ç°ä¸­çš„å‚æ•°å†²çªé—®é¢˜ã€‚

## ç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•ç»“æœ

| transformers | tokenizers | peft        | çŠ¶æ€ | é”™è¯¯ç±»å‹                 |
| ------------ | ---------- | ----------- | ---- | ------------------------ |
| 4.53.0.dev0  | 0.21.1     | 0.16.1.dev0 | âŒ   | `inputs_embeds` å¤šé‡å€¼   |
| 4.44.0       | 0.19.1     | 0.16.1.dev0 | âŒ   | `inputs_embeds` æœªçŸ¥å‚æ•° |
| 4.37.0       | 0.15.2     | 0.16.1.dev0 | âŒ   | tokenizer ä¸å…¼å®¹         |
| 4.40.0       | 0.19.1     | 0.16.1.dev0 | âŒ   | PEFT å¯¼å…¥é”™è¯¯            |
| 4.44.0       | 0.19.1     | 0.8.2       | âŒ   | `inputs_embeds` æœªçŸ¥å‚æ•° |
| 4.28.0       | 0.13.2     | 0.8.2       | âŒ   | InstructBLIP ä¸å­˜åœ¨      |

## é‡è¦å‘ç°ï¼šRSGPT çš„çœŸå®å®ç°

ç»è¿‡æ·±å…¥åˆ†æ RSGPT ä»£ç ï¼Œå‘ç°äº†å…³é”®äº‹å®ï¼š

1. **RSGPT å¹¶æœªä½¿ç”¨ LAVIS**ï¼šä½¿ç”¨çš„æ˜¯ transformers 4.28.0 + æ‰‹åŠ¨å®ç°
2. **transformers 4.28.0 æ²¡æœ‰ InstructBLIP**ï¼šInstructBLIP æ˜¯åæ¥æ‰åŠ å…¥çš„
3. **RSGPT ä½¿ç”¨ LLaMA è€Œé T5**ï¼šé¿å…äº† T5 çš„ `inputs_embeds` å†²çª
4. **æ‰‹åŠ¨å®ç° forward æµç¨‹**ï¼šå®Œå…¨ç»•è¿‡ HF InstructBLIP çš„å‚æ•°ä¼ é€’é—®é¢˜

### RSGPT çš„æˆåŠŸå…³é”®

```python
# RSGPT æ‰‹åŠ¨æ„å»º inputs_embedsï¼Œåªä¼ é€’ç»™ LLaMA
inputs_embeds = self.llm_model.get_input_embeddings()(llm_tokens['input_ids'])
inputs_embeds = torch.cat([inputs_llm, inputs_embeds], dim=1)

outputs = self.llm_model(
    inputs_embeds=inputs_embeds,  # åªä¼ é€’è¿™ä¸ªï¼Œä¸ä¼ é€’ input_ids
    attention_mask=attention_mask,
    labels=targets,
)
```

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: æ‰‹åŠ¨å®ç° InstructBLIP Forward â­ **å½“å‰å°è¯•**

åŸºäº RSGPT çš„æˆåŠŸç»éªŒï¼Œæ‰‹åŠ¨å®ç° forward æµç¨‹é¿å¼€ HF å‚æ•°å†²çªï¼š

```python
def custom_forward(self, batch):
    # 1. å›¾åƒç¼–ç  (ä½¿ç”¨ HF InstructBLIP çš„ vision_model)
    image_embeds = self.model.vision_model(batch['pixel_values'])

    # 2. Q-Former å¤„ç† (åº”ç”¨ LoRA)
    query_outputs = self.model.qformer(...)

    # 3. æ‰‹åŠ¨æ„å»º inputs_embeds
    inputs_embeds = self.model.language_model.get_input_embeddings()(input_ids)
    inputs_embeds = torch.cat([query_outputs, inputs_embeds], dim=1)

    # 4. ç›´æ¥è°ƒç”¨ T5ï¼Œåªä¼ é€’ inputs_embeds
    outputs = self.model.language_model(
        inputs_embeds=inputs_embeds,  # åªä¼ é€’è¿™ä¸ª
        attention_mask=attention_mask,
        labels=labels
    )
```

**çŠ¶æ€**: ğŸ”„ **å®ç°ä¸­**

#### å°è¯• 1: åŸºç¡€è‡ªå®šä¹‰ forward

- **é”™è¯¯**: `'BaseModelOutputWithPooling' object has no attribute 'size'`
- **åŸå› **: vision_model è¿”å›è¾“å‡ºå¯¹è±¡ï¼Œéœ€è¦è®¿é—® `.last_hidden_state`
- **ä¿®å¤**: ä½¿ç”¨ `image_embeds.last_hidden_state` è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ `image_embeds`
- **çŠ¶æ€**: âœ… **å·²ä¿®å¤**

#### å°è¯• 2: Q-Former æ¥å£é—®é¢˜

- **é”™è¯¯**: `'InstructBlipQFormerModel' object has no attribute 'bert'`
- **åŸå› **: HF InstructBLIP çš„ Q-Former ç»“æ„ä¸ RSGPT ä¸åŒ
- **ä¿®å¤**: ç›´æ¥è°ƒç”¨ `self.model.qformer()` è€Œä¸æ˜¯ `self.model.qformer.bert()`
- **çŠ¶æ€**: âœ… **å·²ä¿®å¤**

#### å°è¯• 3: è‡ªå®šä¹‰ forward æˆåŠŸï¼ ğŸ‰

- **ç»“æœ**: âœ… **æˆåŠŸç»•è¿‡ `inputs_embeds` å†²çª**
- **è®­ç»ƒçŠ¶æ€**: æ­£å¸¸è¿è¡Œï¼ŒLoss: 2.0195
- **LoRA åº”ç”¨**: Q-Former ä¸­çš„ LoRA æ­£å¸¸å·¥ä½œ
- **æ€§èƒ½**: Epoch 1 å®Œæˆæ—¶é—´ 3.34s (20 samples)
- **å°é—®é¢˜**: æ ¼å¼åŒ–é”™è¯¯ `unsupported format string passed to tuple.__format__` (éå…³é”®)

#### å°è¯• 4: å®Œæ•´è®­ç»ƒæˆåŠŸï¼ ğŸš€

- **ç»“æœ**: âœ… **å®Œæ•´è®­ç»ƒæµç¨‹æˆåŠŸè¿è¡Œ**
- **è®­ç»ƒå®Œæˆ**: æ‰€æœ‰ 10 ä¸ªè®­ç»ƒæ­¥éª¤æˆåŠŸå®Œæˆ
- **Loss è¶‹åŠ¿**: ä» 2.3047 å¼€å§‹ï¼Œè®­ç»ƒæ­£å¸¸è¿›è¡Œ
- **å†…å­˜ä½¿ç”¨**: 8.59GB GPU å†…å­˜ï¼Œæ­£å¸¸èŒƒå›´
- **LoRA æ•ˆæœ**: Q-Former å‚æ•°æ­£å¸¸æ›´æ–°
- **å‰©ä½™é—®é¢˜**:
  - `save_checkpoint` æ–¹æ³•ç¼ºå¤± (å®¹æ˜“ä¿®å¤)
  - éªŒè¯æŸå¤±ä¸º `nan` (æ•°æ®å¤„ç†é—®é¢˜)

### æ–¹æ¡ˆ 2: ä½¿ç”¨ LAVIS å®˜æ–¹å®ç°

**å·²éªŒè¯**: LAVIS å¯èƒ½æœ‰ç›¸åŒçš„ `inputs_embeds` é—®é¢˜ï¼Œä¸” RSGPT å®é™…æœªä½¿ç”¨ LAVIS

### æ–¹æ¡ˆ 3: å¯»æ‰¾å…¼å®¹ç‰ˆæœ¬ç»„åˆ

**å·²éªŒè¯**: æµ‹è¯•äº† 6 ä¸ªç‰ˆæœ¬ç»„åˆï¼Œéƒ½å­˜åœ¨ä¸åŒçš„å…¼å®¹æ€§é—®é¢˜

## ç›¸å…³èµ„æº

1. **LAVIS å®˜æ–¹å®ç°**: [Salesforce LAVIS InstructBLIP](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
2. **æˆåŠŸçš„ LoRA å®ç°**: [dataloop.ai InstructBLIP LoRA](https://dataloop.ai/library/model/noyhanan_instructblip-vicuna-7b-peft-lora/)
3. **å­¦æœ¯è®ºæ–‡**: "Parameter-Efficient Fine-tuning of InstructBLIP for Visual Reasoning Tasks" (NeurIPS 2023 ENLSP Workshop)
4. **RSGPT å®ç°**: åŸºäº LAVIS çš„æ‰‹åŠ¨æ„å»º InstructBLIP forward æµç¨‹
5. **LAVIS æ¨¡å‹æ–‡ä»¶**: `lavis/models/blip2_models/blip2_t5_instruct.py`

## æœ€ç»ˆè§£å†³æ–¹æ¡ˆ âœ…

**æˆåŠŸå®ç°äº†è‡ªå®šä¹‰ forward æ–¹æ³•ï¼Œå®Œå…¨è§£å†³äº† `inputs_embeds` å†²çªé—®é¢˜ï¼**

### å…³é”®æˆåŠŸè¦ç´ 

1. **æ‰‹åŠ¨å®ç° InstructBLIP forward æµç¨‹**:

   - åˆ†æ­¥éª¤å¤„ç†ï¼šVision â†’ Q-Former â†’ æŠ•å½± â†’ æ–‡æœ¬åµŒå…¥ â†’ è¿æ¥ â†’ T5
   - åªå‘ T5 ä¼ é€’ `inputs_embeds`ï¼Œä¸ä¼ é€’ `input_ids`
   - é¿å…äº† HF InstructBLIP çš„å‚æ•°å†²çª

2. **LoRA æ­£å¸¸å·¥ä½œ**:

   - Q-Former ä¸­çš„ LoRA å‚æ•°æ­£å¸¸æ›´æ–°
   - è®­ç»ƒå‚æ•°ï¼š4.84M (0.12% çš„æ€»å‚æ•°)
   - ç¬¦åˆè®ºæ–‡ä¸­çš„ "< 2%" é…ç½®

3. **è®­ç»ƒæ€§èƒ½è‰¯å¥½**:
   - GPU å†…å­˜ä½¿ç”¨ï¼š8.59GB (åˆç†èŒƒå›´)
   - è®­ç»ƒé€Ÿåº¦ï¼š20 samples å®Œæˆæ—¶é—´ < 4s
   - Loss å€¼æ­£å¸¸ï¼šä» 2.3047 å¼€å§‹ä¸‹é™

### æŠ€æœ¯ç»†èŠ‚

```python
# æˆåŠŸçš„è‡ªå®šä¹‰ forward å®ç°
def forward(self, batch):
    # 1. Vision encoding
    vision_outputs = self.model.vision_model(pixel_values)
    image_embeds = vision_outputs.last_hidden_state

    # 2. Q-Former processing (LoRA è‡ªåŠ¨åº”ç”¨)
    query_outputs = self.model.qformer(...)

    # 3. æŠ•å½±å’Œè¿æ¥
    language_model_inputs = self.model.language_projection(query_embeds)
    inputs_embeds = torch.cat([language_model_inputs, text_embeds], dim=1)

    # 4. åªä¼ é€’ inputs_embeds ç»™ T5
    outputs = self.model.language_model(
        inputs_embeds=inputs_embeds,  # å…³é”®ï¼šåªä¼ é€’è¿™ä¸ª
        attention_mask=full_attention_mask,
        labels=labels
    )
```

## ç»“è®º

`inputs_embeds` å†²çªæ˜¯ Hugging Face InstructBLIP å®ç°ä¸­çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜ï¼Œä¸»è¦ç”±äºï¼š

1. ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
2. å‚æ•°ä¼ é€’çš„å¤æ‚æ€§
3. T5 æ¨¡å‹çš„å‚æ•°çº¦æŸ

**âœ… å·²é€šè¿‡æ‰‹åŠ¨å®ç° forward æ–¹æ³•å®Œå…¨è§£å†³ï¼** è¿™ç§æ–¹æ³•ä¸ä»…è§£å†³äº†å†²çªé—®é¢˜ï¼Œè¿˜ä¿æŒäº† LoRA çš„æ­£å¸¸åŠŸèƒ½å’Œè®­ç»ƒæ€§èƒ½ã€‚
