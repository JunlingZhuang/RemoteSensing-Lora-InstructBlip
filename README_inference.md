# æ¨ç†ä½¿ç”¨æŒ‡å— / Inference Guide

## æ¦‚è¿°

æœ¬æ¨ç†æ¨¡å—æ”¯æŒä¸‰ç§æ¨¡å‹ç±»å‹çš„æ¨ç†ï¼š
- **Native InstructBLIP**: åŸç”Ÿçš„InstructBLIPæ¨¡å‹
- **Native BLIP-2**: åŸç”Ÿçš„BLIP-2æ¨¡å‹  
- **LoRA Fine-tuned**: LoRAå¾®è°ƒåçš„InstructBLIPæ¨¡å‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å•å¼ å›¾ç‰‡æ¨ç†

```bash
# ä½¿ç”¨åŸç”ŸInstructBLIP
python inference_demo.py -i image.jpg

# ä½¿ç”¨LoRAå¾®è°ƒæ¨¡å‹
python inference_demo.py -i image.jpg --model lora --lora-path ./saved_models/best_model

# ä½¿ç”¨BLIP-2
python inference_demo.py -i image.jpg --model blip2
```

### 2. æ‰¹é‡å›¾ç‰‡æ¨ç†

```bash
# æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡
python inference_demo.py --batch ./test_images --model lora --lora-path ./saved_models/best_model -o results.json
```

### 3. æ¨¡å‹å¯¹æ¯”

```bash
# å¯¹æ¯”æ‰€æœ‰å¯ç”¨æ¨¡å‹
python inference_demo.py -i image.jpg --model all --lora-path ./saved_models/best_model --compare -o comparison.json
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å‚æ•° (ä½¿ç”¨debug notebookä¸­çš„æˆåŠŸé…ç½®)

```bash
# è‡ªå®šä¹‰ç”Ÿæˆå‚æ•° (é»˜è®¤æ¥è‡ªæˆåŠŸçš„debug_instructblip.ipynbé…ç½®)
python inference_demo.py -i image.jpg \
    --max-tokens 300 \
    --num-beams 1 \
    --temperature 1.0 \
    --top-p 0.9 \
    --repetition-penalty 1.0 \
    --instruction "è¯¦ç»†æè¿°è¿™å¼ é¥æ„Ÿå›¾åƒä¸­çš„åœ°ç‰©ç‰¹å¾"
```

### Python API ä½¿ç”¨

```python
from module.inference.inferencer import ModelInferencer, quick_inference

# 1. å¿«é€Ÿæ¨ç† (ä½¿ç”¨é»˜è®¤çš„æˆåŠŸé…ç½®)
caption = quick_inference("image.jpg", model_type="instructblip")
print(caption)

# 2. è¯¦ç»†æ¨ç† - è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°
inferencer = ModelInferencer(model_type="lora", model_path="./saved_models/best_model")

# ä½¿ç”¨debug notebookä¸­æˆåŠŸçš„é…ç½®
caption = inferencer.generate_caption(
    "image.jpg", 
    "æè¿°è¿™å¼ å›¾åƒ",
    max_new_tokens=300,
    num_beams=1,
    do_sample=True,
    top_p=0.9,
    repetition_penalty=1.0,
    temperature=1.0
)

# 3. æ›´æ–°é»˜è®¤ç”Ÿæˆé…ç½®
inferencer.update_generation_config(
    max_new_tokens=200,
    temperature=0.8
)

# 4. æ‰¹é‡æ¨ç†
results = inferencer.batch_inference(["img1.jpg", "img2.jpg"])

# 5. æ¨¡å‹å¯¹æ¯”
native_inferencer = ModelInferencer(model_type="instructblip")
lora_inferencer = ModelInferencer(model_type="lora", model_path="./saved_models/best_model")
comparison = native_inferencer.compare_models("image.jpg", lora_inferencer)
```

## ğŸ“Š æ¨¡å‹å¯¹æ¯”ç¤ºä¾‹

è¿è¡Œæ¨¡å‹å¯¹æ¯”åï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š

```
ğŸ–¼ï¸ Image: remote_sensing_image.jpg
ğŸ“ Instruction: Describe this remote sensing image in detail.

ğŸ¤– INSTRUCTBLIP:
   This image shows an aerial view of a residential area with buildings and roads.

ğŸ¤– BLIP2:
   a photo of buildings and roads from above

ğŸ¤– LORA:
   This remote sensing image captures a dense residential area with approximately 
   25 houses arranged in a grid pattern. The houses have red and brown rooftops, 
   surrounded by green vegetation. A main road runs through the center from north to south.
```

## ğŸ”„ æ¨¡å‹åŠ è½½æ–¹å¼å·®å¼‚

### LoRAæ¨¡å‹åŠ è½½ (ä¸¤æ­¥åŠ è½½)
1. é¦–å…ˆåŠ è½½base model (InstructBLIP)
2. ç„¶ååŠ è½½LoRAæƒé‡å¹¶åˆå¹¶

```python
# å†…éƒ¨å®ç°
base_model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-flan-t5-xl")
model = PeftModel.from_pretrained(base_model, lora_path)
```

### åŸç”Ÿæ¨¡å‹åŠ è½½ (ç›´æ¥åŠ è½½)
```python
# ç›´æ¥åŠ è½½
model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-flan-t5-xl")
```

## ğŸ“ è¾“å‡ºæ ¼å¼

### å•å¼ æ¨ç†è¾“å‡º
```json
{
  "image_path": "image.jpg",
  "model_type": "lora",
  "instruction": "Describe this image",
  "caption": "Generated caption text",
  "config": {
    "max_new_tokens": 300,
    "temperature": 1.0
  }
}
```

### æ‰¹é‡æ¨ç†è¾“å‡º
```json
{
  "batch_directory": "./test_images",
  "model_type": "lora", 
  "total_images": 10,
  "successful": 9,
  "results": [
    {
      "image_path": "img1.jpg",
      "caption": "Caption for image 1",
      "success": true
    },
    {
      "image_path": "img2.jpg", 
      "error": "File not found",
      "success": false
    }
  ]
}
```

### æ¨¡å‹å¯¹æ¯”è¾“å‡º
```json
{
  "image_path": "image.jpg",
  "instruction": "Describe this image",
  "models_compared": ["instructblip", "lora"],
  "results": {
    "instructblip": "Native model caption",
    "lora": "LoRA model caption"
  }
}
```

## âš™ï¸ é…ç½®å‚æ•°

**é‡è¦å˜æ›´**: Generation settingsç°åœ¨ä»Configä¸­åˆ†ç¦»ï¼Œæ¨ç†æ—¶å¯ä»¥çµæ´»è°ƒæ•´

### é»˜è®¤ç”Ÿæˆå‚æ•° (æ¥è‡ªæˆåŠŸçš„debug_instructblip.ipynb)

```python
# è¿™äº›æ˜¯å†…ç½®çš„é»˜è®¤å€¼ï¼ŒåŸºäºä½ notebookä¸­æˆåŠŸçš„é…ç½®
generation_config = {
    "max_new_tokens": 300,        # æœ€å¤§ç”Ÿæˆtokenæ•°
    "num_beams": 1,               # beam searchæ•°é‡ 
    "do_sample": True,            # æ˜¯å¦é‡‡æ ·
    "top_p": 0.9,                 # top-pé‡‡æ ·
    "temperature": 1.0,           # ç”Ÿæˆæ¸©åº¦
    "repetition_penalty": 1.0     # é‡å¤æƒ©ç½š
}
```

### æ¨ç†æ—¶è‡ªå®šä¹‰å‚æ•°

```python
# æ–¹æ³•1: åœ¨generate_captionæ—¶ä¼ é€’å‚æ•°
caption = inferencer.generate_caption(
    "image.jpg", 
    instruction="æè¿°å›¾åƒ",
    max_new_tokens=200,           # è¦†ç›–é»˜è®¤å€¼
    temperature=0.8               # è¦†ç›–é»˜è®¤å€¼
)

# æ–¹æ³•2: æ›´æ–°é»˜è®¤é…ç½®
inferencer.update_generation_config(
    max_new_tokens=200,
    temperature=0.8
)
```

### Configç±»ç°åœ¨åªåŒ…å«

- æ¨¡å‹è®¾ç½® (model_name, deviceç­‰)
- LoRAè®¾ç½® (lora_r, lora_alphaç­‰) 
- è®­ç»ƒè®¾ç½® (learning_rate, batch_sizeç­‰)
- æ•°æ®è®¾ç½® (æ•°æ®è·¯å¾„, splitç­‰)

## ğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½

```bash
# è¿è¡Œæ¨ç†æ¨¡å—æµ‹è¯•
python module/tests/test_inference.py

# æˆ–ä½¿ç”¨æµ‹è¯•è¿è¡Œå™¨
python module/tests/run_tests.py --pattern inference
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **LoRAæ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®ä¿LoRAæƒé‡æ–‡ä»¶å­˜åœ¨

2. **æ˜¾å­˜ä¸è¶³**
   - å‡å°‘batch_size
   - ä½¿ç”¨CPUæ¨ç†ï¼šè®¾ç½®device="cpu"

3. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ä½¿ç”¨é•œåƒæºæˆ–æœ¬åœ°æ¨¡å‹

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥GPUå†…å­˜
from module.utils import print_gpu_summary
print_gpu_summary()
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨GPUåŠ é€Ÿ**: ç¡®ä¿CUDAå¯ç”¨
2. **æ‰¹é‡å¤„ç†**: å¯¹å¤šå¼ å›¾ç‰‡ä½¿ç”¨batch_inference
3. **æ¨¡å‹ç¼“å­˜**: é¿å…é‡å¤åŠ è½½åŒä¸€æ¨¡å‹
4. **ç²¾åº¦ä¼˜åŒ–**: ä½¿ç”¨float16ç²¾åº¦èŠ‚çœæ˜¾å­˜

## ğŸ¯ åº”ç”¨åœºæ™¯

- **é¥æ„Ÿå›¾åƒåˆ†æ**: è‡ªåŠ¨ç”Ÿæˆé¥æ„Ÿå›¾åƒæè¿°
- **æ¨¡å‹è¯„ä¼°**: å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½
- **æ•°æ®æ ‡æ³¨**: æ‰¹é‡ç”Ÿæˆå›¾åƒæ ‡æ³¨
- **ç ”ç©¶å®éªŒ**: è¯„ä¼°å¾®è°ƒæ•ˆæœ