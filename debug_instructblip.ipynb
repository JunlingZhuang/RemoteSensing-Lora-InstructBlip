{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Transformers version: 4.53.2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Total GPU Memory: 24.00GB\n",
      "Allocated GPU Memory: 0.00GB\n",
      "Reserved GPU Memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "# 1. 检查环境和GPU状态\n",
    "import torch\n",
    "import transformers\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # 使用正确的内存查询API\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\")\n",
    "    print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n",
    "    print(f\"Reserved GPU Memory: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleanup: 0.00GB\n",
      "Reserved Memory after cleanup: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "# 2. 清理GPU内存\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Allocated Memory after cleanup: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n",
    "    print(f\"Reserved Memory after cleanup: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Salesforce/instructblip-flan-t5-xl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f0b9fac1e458d8e978b09dc356392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 3. 尝试使用最小的InstructBLIP模型\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 使用flan-t5-xl版本（更小）\n",
    "model_name = \"Salesforce/instructblip-flan-t5-xl\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# 使用float16减少内存使用\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "processor = InstructBlipProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda\n",
      " Memory after model loading: 9.10GB\n"
     ]
    }
   ],
   "source": [
    "# 4. 将模型移到GPU并检查内存\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Model on {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\" Memory after model loading: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inputs...\n",
      "Inputs prepared\n"
     ]
    }
   ],
   "source": [
    "# 5. 测试简单的图像\n",
    "# url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n",
    "image = Image.open(\"./RSGPT/dataset/RSIEval/images/P1384_0054.png\").convert(\"RGB\")\n",
    "# image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "prompt = \"All the cars on the road?\"\n",
    "\n",
    "# 准备输入\n",
    "print(\"Processing inputs...\")\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"Inputs prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation...\n",
      "Generation completed in 0.14 seconds\n",
      "\n",
      "Generated text: yes\n"
     ]
    }
   ],
   "source": [
    "# 6. 使用最简单的生成参数\n",
    "print(\"Starting generation...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# # 设置超时\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=150,  # 使用max_new_tokens而不是max_length\n",
    "#         do_sample=False,    # 关闭采样\n",
    "#         num_beams=1         # 使用贪婪解码\n",
    "#     )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(\n",
    "#       **inputs,\n",
    "#       max_new_tokens=80,\n",
    "#       do_sample=False,\n",
    "#       num_beams=3,\n",
    "#       repetition_penalty=1.2,\n",
    "#       no_repeat_ngram_size=3,\n",
    "#       early_stopping=True  # 遇到结束符就停止\n",
    "#   )\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#      outputs = model.generate(\n",
    "#       **inputs,\n",
    "#       max_new_tokens=50,\n",
    "#       do_sample=True,  # 开启采样\n",
    "#       temperature=0.7,  # 控制随机性\n",
    "#       top_p=0.9,  # nucleus采样\n",
    "#       repetition_penalty=1.5\n",
    "#   )\n",
    "     \n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "      **inputs,\n",
    "      max_new_tokens=300,  # RSGPT用这个\n",
    "      num_beams=1,         # RSGPT对话用1\n",
    "      do_sample=True,      # RSGPT对话用True\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.0,  # RSGPT对话用1.0\n",
    "      temperature=1.0\n",
    "  )\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Generation completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 解码输出\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(f\"\\nGenerated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
