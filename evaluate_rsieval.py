#!/usr/bin/env python3
"""
Evaluate models on RSIEval VQA dataset.

USAGE COMMANDS:

1. Evaluate native InstructBLIP (baseline):
   python evaluate_rsieval.py --model-type instructblip

2. Evaluate native BLIP-2 (baseline):
   python evaluate_rsieval.py --model-type blip2

3. Evaluate custom LoRA model:
   python evaluate_rsieval.py --model-type lora --lora-checkpoint path/to/checkpoint.pth

4. Quick test with limited samples:
   python evaluate_rsieval.py --model-type lora --lora-checkpoint path/to/checkpoint.pth --max-samples 50

5. Custom output path:
   python evaluate_rsieval.py --model-type lora --lora-checkpoint path/to/checkpoint.pth -o results/my_results.json

6. Custom RSIEval dataset path:
   python evaluate_rsieval.py --rsieval-path /path/to/RSIEval --model-type instructblip

EXAMPLES:

# Evaluate Ultra Conservative LoRA model
python evaluate_rsieval.py --model-type lora --lora-checkpoint checkpoints/ultra_conservative_epoch_10.pth

# Quick test of V5 Memory Optimized model
python evaluate_rsieval.py --model-type lora --lora-checkpoint checkpoints/grid_v5_memory_optimized_epoch_12.pth --max-samples 100

# Compare with native InstructBLIP baseline
python evaluate_rsieval.py --model-type instructblip --max-samples 100

OUTPUT:
- Accuracy by question type (scene, object, attribute, etc.)
- Overall accuracy percentage
- Sample VQA results
- Detailed JSON results file

"""

import os
import json
import argparse
import sys
import torch
from pathlib import Path
from tqdm import tqdm

# Add module to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'module'))

from config import Config
from data.rsicap_dataset import load_rsieval_vqa_data
from inference.inferencer import ModelInferencer


def fix_lora_adapter_config(checkpoint_path):
    """
    ç»Ÿä¸€ä¿®å¤ LoRA adapter_config.json æ–‡ä»¶ä¸­çš„ä¸å…¼å®¹å‚æ•°

    Args:
        checkpoint_path: LoRA checkpoint è·¯å¾„
    """
    adapter_config_path = os.path.join(checkpoint_path, "adapter_config.json")
    if not os.path.exists(adapter_config_path):
        print(f"âš ï¸  adapter_config.json not found at {adapter_config_path}")
        return False

    try:
        # è¯»å–å½“å‰é…ç½®
        with open(adapter_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # å®šä¹‰éœ€è¦ç§»é™¤çš„ä¸å…¼å®¹å‚æ•°
        invalid_params = [
            'corda_config', 'eva_config', 'exclude_modules', 'layer_replication',
            'layers_pattern', 'layers_to_transform', 'megatron_config', 'megatron_core',
            'qalora_group_size', 'trainable_token_indices', 'use_dora', 'use_qalora',
            'use_rslora', 'loftq_config', 'alpha_pattern', 'rank_pattern', 'lora_bias'
        ]

        # æ£€æŸ¥å¹¶ç§»é™¤æ— æ•ˆå‚æ•°
        removed_params = []
        for param in invalid_params:
            if param in config:
                removed_params.append(param)
                del config[param]

        # If parameters were removed, save the fixed configuration
        if removed_params:
            print(f"Fixing adapter_config.json...")
            print(f"   Removing invalid parameters: {', '.join(removed_params)}")

            # Backup original file
            backup_path = adapter_config_path + ".backup"
            if not os.path.exists(backup_path):
                import shutil
                shutil.copy2(adapter_config_path, backup_path)
                print(f"   Backup saved to: {backup_path}")

            # Save fixed configuration
            with open(adapter_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)

            print(f"adapter_config.json fixed successfully!")
            return True
        else:
            print(f"adapter_config.json is already compatible")
            return False

    except Exception as e:
        print(f"Error fixing adapter_config.json: {e}")
        return False


def fix_all_lora_configs_in_directory(checkpoint_dir):
    """
    Fix all possible LoRA configuration files in directory

    Args:
        checkpoint_dir: checkpoint directory path
    """
    print(f"Scanning for LoRA configs in: {checkpoint_dir}")

    if not os.path.exists(checkpoint_dir):
        print(f"Checkpoint directory not found: {checkpoint_dir}")
        return

    fixed_count = 0

    # Traverse all subdirectories and files in the directory
    for root, dirs, files in os.walk(checkpoint_dir):
        for file in files:
            if file == "adapter_config.json":
                config_path = os.path.join(root, file)
                print(f"   Found adapter_config.json: {config_path}")

                # Fix this configuration file
                parent_dir = os.path.dirname(config_path)
                if fix_lora_adapter_config(parent_dir):
                    fixed_count += 1

    if fixed_count > 0:
        print(f"Fixed {fixed_count} LoRA configuration file(s)")
    else:
        print(f"All LoRA configurations are compatible")


def evaluate_model_vqa_on_rsieval(rsieval_path, model_type="instructblip", lora_checkpoint=None, output_path=None, max_samples=None):
    """Evaluate models on RSIEval VQA dataset
    
    Args:
        rsieval_path: Path to RSIEval dataset
        model_type: One of ["instructblip", "blip2", "lora"]
        lora_checkpoint: Path to LoRA checkpoint (required if model_type="lora")
        output_path: Output JSON file path
        max_samples: Limit number of samples for testing
    """
    
    # Create config
    config = Config()
    
    print("Loading RSIEval VQA dataset...")
    test_loader, processor, qa_samples = load_rsieval_vqa_data(config, rsieval_path)
    
    if max_samples:
        # Limit samples for testing
        limited_samples = qa_samples[:max_samples]
        print(f"Limiting to {max_samples} QA pairs for testing")
    else:
        limited_samples = qa_samples
    
    # Initialize model based on type
    if model_type == "lora":
        if not lora_checkpoint:
            raise ValueError("LoRA checkpoint path required for model_type='lora'")

        # Uniformly fix LoRA configuration files
        print(f"Checking LoRA checkpoint: {lora_checkpoint}")

        # If it's a single checkpoint, fix directly
        if os.path.isdir(lora_checkpoint):
            fix_lora_adapter_config(lora_checkpoint)
        else:
            # If it's the parent directory of checkpoint directory, scan all subdirectories
            checkpoint_parent = os.path.dirname(lora_checkpoint)
            if os.path.exists(checkpoint_parent):
                fix_all_lora_configs_in_directory(checkpoint_parent)

        print(f"Initializing LoRA model from checkpoint: {lora_checkpoint}")
        inferencer = ModelInferencer(model_type="lora", model_path=lora_checkpoint)
        model_name = f"lora_{os.path.basename(lora_checkpoint)}"
    elif model_type == "blip2":
        print("Initializing native BLIP-2 model...")
        inferencer = ModelInferencer(model_type="blip2")
        model_name = "native_blip2"
    else:  # default to instructblip
        print("Initializing native InstructBLIP model...")
        inferencer = ModelInferencer(model_type="instructblip")
        model_name = "native_instructblip"
    
    results = []
    correct_answers = 0
    
    # Track accuracy by question type
    type_stats = {}
    
    print(f"Running VQA inference on {len(limited_samples)} QA pairs...")
    for i, sample in enumerate(tqdm(limited_samples)):
        image_path = os.path.join(test_loader.dataset.images_dir, sample['filename'])
        
        if not os.path.exists(image_path):
            print(f"Warning: Image not found: {image_path}")
            continue
        
        try:
            # Generate answer using the question as prompt
            generated_answer = inferencer.generate_caption(
                image_path,
                sample['question'],  # Use question as prompt
                max_new_tokens=300,  # RSGPTç”¨è¿™ä¸ª
                num_beams=1,         # RSGPTå¯¹è¯ç”¨1
                do_sample=True,      # RSGPTå¯¹è¯ç”¨True
                top_p=0.9,
                repetition_penalty=1.0,  # RSGPTå¯¹è¯ç”¨1.0
                temperature=1.0
            )

            # Clear GPU cache periodically to prevent memory accumulation
            if (i + 1) % 50 == 0:
                torch.cuda.empty_cache()
                print(f"ðŸ§¹ GPU cache cleared at sample {i + 1}")
            
            # Simple answer matching (case-insensitive)
            # Remove punctuation for better matching
            gt_clean = sample['answer'].lower().strip().rstrip('.')
            gen_clean = generated_answer.lower().strip()
            is_correct = gt_clean in gen_clean
            if is_correct:
                correct_answers += 1
            
            # Update type statistics
            question_type = sample['type']
            if question_type not in type_stats:
                type_stats[question_type] = {'correct': 0, 'total': 0}
            type_stats[question_type]['total'] += 1
            if is_correct:
                type_stats[question_type]['correct'] += 1
            
            result = {
                "sample_id": i,
                "image_file": sample['filename'],
                "question": sample['question'],
                "ground_truth_answer": sample['answer'],
                "generated_answer": generated_answer,
                "question_type": sample['type'],
                "is_correct": is_correct
            }
            results.append(result)
            
            if (i + 1) % 10 == 0:
                accuracy = correct_answers / len(results) * 100
                print(f"Processed {i + 1}/{len(limited_samples)} QA pairs")
                print(f"Current accuracy: {accuracy:.2f}%")
                print(f"Latest Q: {sample['question']}")
                print(f"Ground truth: {sample['answer']}")
                print(f"Generated: {generated_answer}")
                print("-" * 40)
        
        except Exception as e:
            print(f"Error processing sample {i} ({sample['filename']}): {e}")
            print(f"   Question: {sample['question']}")

            # Save partial results if we've processed a significant number
            if len(results) >= 50 and (i + 1) % 100 == 0:
                partial_output_path = output_path.replace('.json', f'_partial_{i+1}.json') if output_path else f"partial_results_{i+1}.json"
                print(f"Saving partial results to {partial_output_path}")

                partial_data = {
                    "model_type": model_name,
                    "dataset": "RSIEval_VQA",
                    "processed_samples": i + 1,
                    "successful_samples": len(results),
                    "partial_accuracy": correct_answers / len(results) * 100 if results else 0,
                    "results": results
                }

                with open(partial_output_path, 'w', encoding='utf-8') as f:
                    json.dump(partial_data, f, ensure_ascii=False, indent=2)

            continue
    
    # Calculate final accuracy and type accuracies
    final_accuracy = correct_answers / len(results) * 100 if results else 0
    
    print(f"\nCompleted VQA inference on {len(results)}/{len(limited_samples)} QA pairs")
    print(f"\n{'='*60}")
    print("RESULTS BY QUESTION TYPE")
    print(f"{'='*60}")
    
    type_accuracies = {}
    for qtype, stats in type_stats.items():
        accuracy = stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0
        type_accuracies[qtype] = accuracy
        print(f"{qtype:15s}: {accuracy:6.2f}% ({stats['correct']:3d}/{stats['total']:3d})")
    
    print(f"{'='*60}")
    print(f"Overall Accuracy: {final_accuracy:.2f}% ({correct_answers}/{len(results)})")
    print(f"{'='*60}")
    
    # Save results
    if output_path:
        # Create results directory
        results_dir = os.path.dirname(output_path)
        if results_dir:
            os.makedirs(results_dir, exist_ok=True)
        
        output_data = {
            "model_type": model_name,
            "model_config": {
                "type": model_type,
                "checkpoint": lora_checkpoint if model_type == "lora" else None
            },
            "dataset": "RSIEval_VQA",
            "total_qa_pairs": len(limited_samples),
            "successful_qa_pairs": len(results),
            "overall_accuracy": final_accuracy,
            "correct_answers": correct_answers,
            "accuracy_by_type": type_accuracies,
            "type_statistics": type_stats,
            "generation_config": {
                "max_new_tokens": 300,
                "num_beams": 1,
                "do_sample": True,
                "top_p": 0.9,
                "repetition_penalty": 1.0,
                "temperature": 1.0
            },
            "results": results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nResults saved to {output_path}")
    
    # Show sample results
    print("\n" + "="*60)
    print("SAMPLE VQA RESULTS")
    print("="*60)
    for i, result in enumerate(results[:3]):
        print(f"\nSample {i+1}:")
        print(f"Image: {result['image_file']}")
        print(f"Question: {result['question']}")
        print(f"Ground Truth Answer: {result['ground_truth_answer']}")
        print(f"Generated Answer: {result['generated_answer']}")
        print(f"Correct: {result['is_correct']}")
        print("-" * 40)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Evaluate models on RSIEval VQA dataset")
    
    parser.add_argument(
        "--rsieval-path",
        type=str,
        required=False,
        default=None,
        help="Path to RSIEval dataset directory (default: data/rsgpt_dataset/RSIEval)"
    )
    
    parser.add_argument(
        "--model-type",
        type=str,
        choices=["instructblip", "blip2", "lora"],
        default="instructblip",
        help="Model type to evaluate"
    )
    
    parser.add_argument(
        "--lora-checkpoint",
        type=str,
        help="Path to LoRA checkpoint (required for --model-type lora)"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="Output file for results (auto-generated if not specified)"
    )
    
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Maximum number of QA pairs to evaluate (for testing)"
    )
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.model_type == "lora" and not args.lora_checkpoint:
        print("Error: --lora-checkpoint is required when using --model-type lora")
        sys.exit(1)
    
    # Use default path if not provided
    if args.rsieval_path is None:
        args.rsieval_path = "data/rsgpt_dataset/RSIEval"

    if not os.path.exists(args.rsieval_path):
        print(f"Error: RSIEval path not found: {args.rsieval_path}")
        sys.exit(1)
    
    # Auto-generate output filename if not provided
    if not args.output:
        # Create results directory
        results_dir = "results"
        if args.model_type == "lora":
            checkpoint_name = os.path.splitext(os.path.basename(args.lora_checkpoint))[0]
            args.output = os.path.join(results_dir, f"rsieval_{args.model_type}_{checkpoint_name}_vqa_results.json")
        else:
            args.output = os.path.join(results_dir, f"rsieval_{args.model_type}_vqa_results.json")
    
    try:
        results = evaluate_model_vqa_on_rsieval(
            rsieval_path=args.rsieval_path,
            model_type=args.model_type,
            lora_checkpoint=args.lora_checkpoint,
            output_path=args.output,
            max_samples=args.max_samples
        )
        
        print(f"\nVQA Evaluation completed successfully!")
        print(f"Model: {args.model_type}")
        if args.lora_checkpoint:
            print(f"LoRA checkpoint: {args.lora_checkpoint}")
        print(f"Total QA pairs processed: {len(results)}")
        print(f"Results saved to: {args.output}")
        
    except Exception as e:
        print(f"VQA Evaluation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()